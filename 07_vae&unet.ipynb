{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25d7ba1-702e-4ab2-8e86-289bd48f7e26",
   "metadata": {},
   "source": [
    "# Mess with VAE embeddings and Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1cfe06-dca2-4b1c-8126-32aa5a27dfd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'show' from 'utils.model' (/pfs/data5/home/kn/kn_kn/kn_pop531500/project/utils/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VaeImageProcessor\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VAE, show\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'show' from 'utils.model' (/pfs/data5/home/kn/kn_kn/kn_pop531500/project/utils/model.py)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from utils.model import VAE\n",
    "from utils.show import flipbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a5232-2d16-4848-bd66-74de4f2a9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load components\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True)\n",
    "scheduler = PNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0429ad-d698-49da-965a-d9952091cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to GPU\n",
    "device = \"cuda\"\n",
    "vae.to(device)\n",
    "text_encoder.to(device)\n",
    "unet.to(device)\n",
    "vae = VAE(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28527262-76a9-4c4a-bc08-543eb859e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "prompt = [\"a picture of a red car\", 'a picture of a green airplane']\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f66fd-7d5c-4e25-ac21-1f0a01fd7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text tokens and embeddings \n",
    "def clip(text):\n",
    "    text_input = tokenizer(text, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "    # empty embeddings used for classifier free guidance\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer([\"\"] * len(text), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006cc75-7286-49e4-a17e-8f4603bbf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRnd(size):\n",
    "    latents = torch.randn((size, unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to(device)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adee989-cd04-437e-9cd5-fcd0ace1b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "\n",
    "def gen(start, conditioning, s=num_inference_steps):\n",
    "    latents = start\n",
    "    intermediate = torch.zeros([s+1,4,64,64]).to(device)\n",
    "    scheduler.set_timesteps(s)\n",
    "    for x,t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        latent_model_input = torch.cat([latents] *2)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=conditioning).sample\n",
    "\n",
    "        noise_pred_uncond, noise_pred_text =noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        intermediate[x] = latents.clone()\n",
    "    return [latents, intermediate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505868c-030f-4cb2-8bd1-edf9598ffc61",
   "metadata": {},
   "source": [
    "# Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfe7b6-fefe-46af-9f71-56fc04361748",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = getRnd(1)\n",
    "flipbook(vae.d(noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c4d1c-7631-4eb9-973b-b0fd7e26c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb01 = clip(['A green car'])\n",
    "emb01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98b964-28b4-4769-bf54-9db0f7edc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "l01, steps01 = gen(getRnd(1), emb01, 50)\n",
    "out01 = vae.d(steps01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ea800-3d86-4dcd-ac2c-4213b0444c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipbook(out01, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a4d24-ad75-4b05-8dac-728529ddac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 1/2\n",
    "l02 = (l01[0]*ratio+getRnd(1)[0]*(1-ratio))\n",
    "l02 = l02.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb523e-1981-4072-8d1a-0cb75710c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out02 = vae.d(l02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992afd5-c0d7-4f9a-a57a-ea4271a7b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipbook(out02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc56b2-6323-4eed-830c-4417adaa96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb02 = clip(['a red airplane plane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb564a0-e0f7-4fc6-a11c-1159fa4ea44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l03,steps03 = gen(l02, emb02)\n",
    "#l03,steps03 = gen(l01, emb02)\n",
    "#l03,steps03 = gen(getRnd(1), emb02)\n",
    "out03 = vae.d(steps03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcc79e-9a92-4058-8753-4439045a8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipbook(out03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2721c37-ab37-4916-9472-f525625381c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
